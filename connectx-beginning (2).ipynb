{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install 'kaggle-environments==0.1.6'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\n#env.render()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_key = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]).astype(str) \nprint(int(''.join(state_key)) )#空いてる所消す\nprint(int(''.join(state_key), 3) ) #3進数として解釈して10進に直している\nprint(hex(int(''.join(state_key), 3)))\ns = hex(int(''.join(state_key), 3))[2:]  #0xは切り離す\nprint(state_key, s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([None, \"random\"])\n\nclass RandomAgent:\n    def __init__(self):\n        #self.env = env envはいったん外付けで\n        self.gamma = 1\n        self.action_size = 7\n\n        random_actions = {0:1/7, 1:1/7, 2:1/7, 3:1/7, 4:1/7, 5:1/7, 6:1/7}\n\n        self.pi = defaultdict(lambda: random_actions)\n        self.v = defaultdict(lambda: 0)\n        self.cnts = defaultdict(lambda: 0)\n        self.memory = [] # state_key, action, reward\n        \n    def generate_state_key(self, observation):\n        state = observation.board[:]\n        state.append(observation.mark)\n        state_key = np.array(state).astype(str) \n\n        return hex(int(''.join(state_key), 3))[2:]\n\n    def get_action(self, observation):\n        #print(\"enter\")\n        state = observation.board #埋まっている部分の可能性は他のところに山分けする\n        \n        \n\n        state_key = self.generate_state_key(observation)\n        #おけない部分の修正\n        #山分けは誤差でばグルのでやめる\n        p = 0\n        tmp = {0:-1, 1:-1, 2:-1, 3:-1, 4:-1, 5:-1, 6:-1}\n        for i in range(self.action_size):\n            #print(\"state\", state[i])\n            if state[i] != 0:\n                #print(\"bpi\", self.pi[1])\n                tmp[i] = 0\n                #print(\"pi\", self.pi[1])\n            else:\n                \n                p += 1\n       # print(\"middle\", p)\n        \n        for i in range(self.action_size):\n            if state[i] == 0:\n                tmp[i] = 1 / p\n                \n        self.pi[state_key] = tmp\n        action_probs = self.pi[state_key]\n        \n        \n        actions = list(action_probs.keys())\n        probs = list(action_probs.values())\n        \n        \n        #if p != self.action_size and sum(probs) != 1:\n            #print(\"before\")\n            #print(probs, sum(probs), state, p, self.pi[1])\n            #print(\"after\")\n        \n        #print(\"b\",sum(probs),probs)\n        return np.random.choice(actions, p=probs).tolist()\n\n    def add(self, observation, action, reward):\n        state_key = self.generate_state_key(observation)\n        data = (state_key, action, reward)\n        self.memory.append(data)\n\n    def reset(self):\n        self.memory.clear()\n\n    def eval(self):\n        G = 0\n        for data in reversed(self.memory):\n            state_key, action, reward = data\n            G = self.gamma * G + reward\n            self.cnts[state_key] += 1\n            self.v[state_key] += (G - self.v[state_key]) / self.cnts[state_key]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([None, \"random\"])\n#　遷移確率は無い（完全にa,sに依存）し、\n# 報酬もゲームが終了するか否かの判定によって得られるためQ関数を用いる必然性はないが…\nclass McAgent:\n    def __init__(self):\n        self.gamma = 1\n        self.action_size = 7\n        self.epsilon = 0.1\n        self.alpha = 0.9\n        random_actions = {action: 1/7 for action in range(self.action_size)}\n\n        self.pi = defaultdict(lambda: random_actions)\n        self.Q = defaultdict(lambda: 0)\n        self.cnts = defaultdict(lambda: 0)\n        self.memory = []\n    \n    def generate_state_key(self, observation):\n        state = observation.board[:]\n        state.append(observation.mark)\n        state_key = np.array(state).astype(str) \n\n        return hex(int(''.join(state_key), 3))[2:]\n\n    \n    def get_action(self, observation):\n        #行動はpiから取得\n        state = observation.board #埋まっている部分の可能性は他のところに山分けする\n\n        state_key = self.generate_state_key(observation)\n        #おけない部分の修正\n        #山分けをあきらめない\n        p = 0\n        pool = 0\n        tmp = {0:-1, 1:-1, 2:-1, 3:-1, 4:-1, 5:-1, 6:-1}\n        for i in range(self.action_size):\n           \n            if state[i] != 0:\n                #print(\"captured\")\n                tmp[i] = 0 \n                pool += self.pi[state_key][i]\n            else:\n                p += 1\n        \n        pool = pool / p\n       \n            \n        for i in range(self.action_size):\n            if state[i] == 0:\n                tmp[i] = pool + self.pi[state_key][i]\n            else:\n                tmp[i] = 0\n        \n        #if p != self.action_size:\n            #print(\"challenge\")\n            #print(\"before\", self.pi[state_key])\n            #print(\"after\", tmp)\n       \n                \n        self.pi[state_key] = tmp\n        action_probs = self.pi[state_key]\n        #print(action_probs)\n       \n        \n        actions = list(action_probs.keys())\n        probs = list(action_probs.values())\n        \n        return np.random.choice(actions, p=probs).tolist()\n    \n    def add(self, observation, action, reward):\n        state_key = self.generate_state_key(observation)\n        data = (state_key, action, reward)\n        #print(data)\n        self.memory.append(data)\n\n    def reset(self):\n        self.memory.clear()\n    \n    def greedy_probs(self, Q, state_key):\n        #print(\"greedy\", state_key)\n        p = 0\n        tmp = []\n        for action in range(self.action_size):\n            if self.pi[state_key][action] == 0:\n                tmp.append(0)\n            else:\n                tmp.append(1)\n                p += 1\n        qs = [Q[(state_key, action)] for action in range(self.action_size)]\n        max_action = np.argmax(qs)\n        #おけない場所\n        base_prob = self.epsilon / p\n        #だめそうだったらprob作ってから\n        action_probs = {action: 0.0 for action in range(self.action_size)}\n        for action in range(self.action_size):\n            if tmp[action] == 1:\n                action_probs[action] += base_prob\n        \n        action_probs[max_action] += 1 - self.epsilon\n        #print(\"before greedy\", self.pi[state_key])\n        #print(\"afrer greedy\", action_probs)\n        \n        return action_probs\n    \n    def update(self):\n        G = 0\n        for data in reversed(self.memory):\n            state_key, action, reward = data\n            G = self.gamma * G + reward\n            key = (state_key, action)\n            self.cnts[key] += 1 \n            self.Q[key] += (G - self.Q[key]) * self.alpha\n            self.pi[state_key] = self.greedy_probs(self.Q, state_key)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-21T09:09:05.115185Z","iopub.execute_input":"2023-02-21T09:09:05.115634Z","iopub.status.idle":"2023-02-21T09:09:05.202626Z","shell.execute_reply.started":"2023-02-21T09:09:05.115598Z","shell.execute_reply":"2023-02-21T09:09:05.201613Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([None, \"random\"])\n#　遷移確率は無い（完全にa,sに依存）し、\n# 報酬もゲームが終了するか否かの判定によって得られるためQ関数を用いる必然性はないが…\nclass McOffPolicyAgent:\n    def __init__(self):\n        self.gamma = 1\n        self.action_size = 7\n        self.epsilon = 0.1\n        self.alpha = 0.9\n        random_actions = {action: 1/7 for action in range(self.action_size)}\n        self.pi = defaultdict(lambda: random_actions)\n        self.b = defaultdict(lambda: random_actions)\n        self.Q = defaultdict(lambda: 0)\n        self.cnts = defaultdict(lambda: 0)\n        self.memory = []\n        \n    def generate_state_key(self, observation):\n        state = observation.board[:]\n        state.append(observation.mark)\n        state_key = np.array(state).astype(str) \n\n        return hex(int(''.join(state_key), 3))[2:]\n\n    \n    def get_action(self, observation):\n        #行動はbから取得\n        #最後はpiを提出\n        state = observation.board #埋まっている部分の可能性は他のところに山分けする\n\n        state_key = self.generate_state_key(observation)\n        #おけない部分の修正\n        #山分けをあきらめない\n        p = 0\n        pool = 0\n        tmp = {0:-1, 1:-1, 2:-1, 3:-1, 4:-1, 5:-1, 6:-1}\n        for i in range(self.action_size):\n           \n            if state[i] != 0:\n                #print(\"captured\")\n                tmp[i] = 0 \n                pool += self.b[state_key][i]\n            else:\n                p += 1\n        \n        pool = pool / p\n       \n            \n        for i in range(self.action_size):\n            if state[i] == 0:\n                tmp[i] = pool + self.b[state_key][i]\n            else:\n                tmp[i] = 0\n        \n        #if p != self.action_size:\n            #print(\"challenge\")\n            #print(\"before\", self.b[state_key])\n            #print(\"after\", tmp)\n       \n                \n        self.b[state_key] = tmp\n        action_probs = self.b[state_key]\n        #print(action_probs)\n       \n        \n        actions = list(action_probs.keys())\n        probs = list(action_probs.values())\n        \n        return np.random.choice(actions, p=probs).tolist()\n    \n    def add(self, observation, action, reward):\n        state_key = self.generate_state_key(observation)\n        data = (state_key, action, reward)\n        #print(data)\n        self.memory.append(data)\n\n    def reset(self):\n        self.memory.clear()\n    \n    def greedy_probs(self, Q, state_key, epsilon):\n        #print(\"greedy\", state_key)\n        p = 0\n        tmp = []\n        for action in range(self.action_size):\n            if self.b[state_key][action] == 0:\n                tmp.append(0)\n            else:\n                tmp.append(1)\n                p += 1\n        qs = [Q[(state_key, action)] for action in range(self.action_size)]\n        max_action = np.argmax(qs)\n        #おけない場所\n        base_prob = epsilon / p\n        #だめそうだったらprob作ってから\n        action_probs = {action: 0.0 for action in range(self.action_size)}\n        for action in range(self.action_size):\n            if tmp[action] == 1:\n                action_probs[action] += base_prob\n        \n        action_probs[max_action] += 1 - epsilon\n        print(\"before greedy\", self.pi[state_key])\n        print(\"afrer greedy\", action_probs)\n        \n        return action_probs\n    \n    def update(self):\n        G = 0\n        rho = 1\n        for data in reversed(self.memory):\n            state_key, action, reward = data\n            rho = self.pi[state_key][action]/self.b[state_key][action]\n            G = self.gamma * rho * G + reward\n            key = (state_key, action)\n            self.cnts[key] += 1 \n            self.Q[key] += ( G - self.Q[key]) * self.alpha\n            self.pi[state_key] = self.greedy_probs(self.Q, state_key, 0)\n            self.b[state_key] = self.greedy_probs(self.Q, state_key, self.epsilon)\n            \n            \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:37:07.768764Z","iopub.execute_input":"2023-02-21T10:37:07.769664Z","iopub.status.idle":"2023-02-21T10:37:07.863266Z","shell.execute_reply.started":"2023-02-21T10:37:07.769606Z","shell.execute_reply":"2023-02-21T10:37:07.861876Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([\"random\", None])\nepisodes = 1\nagent = McOffPolicyAgent()\nprint(type(agent))\n\n\nfor episode in range(episodes):\n    #print(\"new game\")\n    agent.reset()\n    observation = trainer.reset()\n    #print(type(observation.board))\n    #env.render()\n    \n    \n    while not env.done:\n        print(\"key\", agent.generate_state_key(observation))\n        action = agent.get_action(observation)\n        #print(type(action.tolist()))\n        #past_observation = observation\n        next_observation, reward, done, info = trainer.step(action)\n        #print(type(reward), reward,done,info)\n      \n        agent.add( observation, action, reward)\n        \n        if done:\n            #print(reward)\n            agent.update()\n            break\n        \n        observation = next_observation\n        #print(observation.board)\n\nprint(agent.pi)\nprint(agent.b)\n#print(agent.memory)\n#print(agent.Q)","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:37:13.051943Z","iopub.execute_input":"2023-02-21T10:37:13.052793Z","iopub.status.idle":"2023-02-21T10:37:13.304032Z","shell.execute_reply.started":"2023-02-21T10:37:13.052743Z","shell.execute_reply":"2023-02-21T10:37:13.302503Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"<class '__main__.McOffPolicyAgent'>\nkey 53\nkey 125c\nkey 12fe39\nkey 4559b21a\nkey 7388d4b5\nkey 13c48fbfe8900a\nkey 13c555063ec05f\nkey 13c555063f4084\nkey 20eff4cd0c94c7\nkey 20eff4cd24e8d0\nkey 11954c7286704c107\nkey 11954c72936daa9ea\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.92, 1: 0.02, 2: 0.0, 3: 0.0, 4: 0.02, 5: 0.02, 6: 0.02}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.02, 1: 0.02, 2: 0.0, 3: 0.0, 4: 0.92, 5: 0.02, 6: 0.02}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.9142857142857143, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.9142857142857143, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.9142857142857143, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.9142857142857143}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.9142857142857143, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.9142857142857143, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.9142857142857143, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.9142857142857143}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.9142857142857143, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\nbefore greedy {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 5: 0.14285714285714285, 6: 0.14285714285714285}\nafrer greedy {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nbefore greedy {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\nafrer greedy {0: 0.9142857142857143, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}\ndefaultdict(<function McOffPolicyAgent.__init__.<locals>.<lambda> at 0x7f2fb6578e60>, {'11954c72936daa9ea': {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}, '11954c7286704c107': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0}, '20eff4cd24e8d0': {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}, '20eff4cd0c94c7': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0}, '13c555063f4084': {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}, '13c555063ec05f': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0}, '13c48fbfe8900a': {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}, '7388d4b5': {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}, '4559b21a': {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0}, '12fe39': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0}, '125c': {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}, '53': {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}})\ndefaultdict(<function McOffPolicyAgent.__init__.<locals>.<lambda> at 0x7f2fb6578d40>, {'53': {0: 0.9142857142857143, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '125c': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.9142857142857143, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '12fe39': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.9142857142857143}, '4559b21a': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.9142857142857143, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '7388d4b5': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.9142857142857143, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '13c48fbfe8900a': {0: 0.014285714285714287, 1: 0.9142857142857143, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '13c555063ec05f': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.9142857142857143}, '13c555063f4084': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.9142857142857143, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '20eff4cd0c94c7': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.014285714285714287, 4: 0.9142857142857143, 5: 0.014285714285714287, 6: 0.014285714285714287}, '20eff4cd24e8d0': {0: 0.014285714285714287, 1: 0.014285714285714287, 2: 0.014285714285714287, 3: 0.9142857142857143, 4: 0.014285714285714287, 5: 0.014285714285714287, 6: 0.014285714285714287}, '11954c7286704c107': {0: 0.02, 1: 0.02, 2: 0.0, 3: 0.0, 4: 0.92, 5: 0.02, 6: 0.02}, '11954c72936daa9ea': {0: 0.92, 1: 0.02, 2: 0.0, 3: 0.0, 4: 0.02, 5: 0.02, 6: 0.02}})\n","output_type":"stream"}]},{"cell_type":"code","source":"#print(agent.pi)","metadata":{"execution":{"iopub.status.busy":"2023-02-21T09:34:41.234350Z","iopub.execute_input":"2023-02-21T09:34:41.234770Z","iopub.status.idle":"2023-02-21T09:34:41.240573Z","shell.execute_reply.started":"2023-02-21T09:34:41.234735Z","shell.execute_reply":"2023-02-21T09:34:41.239391Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"pi = agent.pi\np = {}\n\n    \ndef my_agent(observation, configuration):\n    import numpy as np\n    from random import choice\n    from collections import defaultdict\n    pi = agent.pi # あとで変換\n    state = observation.board[:]\n    state.append(observation.mark)\n    state_key = np.array(state).astype(str) \n    state_key = hex(int(''.join(state_key), 3))[2:]\n    print(state_key)\n    if state_key not in pi.keys():\n        print(\"random\")\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    \n    actions = list(pi[state_key].keys())\n    probs = list(pi[state_key].values())\n    action = np.random.choice(actions, p=probs).tolist()\n    \n    return action\n\n#print(dict(agent.pi))","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:12:44.532136Z","iopub.execute_input":"2023-02-21T10:12:44.532578Z","iopub.status.idle":"2023-02-21T10:12:44.541821Z","shell.execute_reply.started":"2023-02-21T10:12:44.532545Z","shell.execute_reply":"2023-02-21T10:12:44.540951Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import sys\nout = sys.stdout\n\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([my_agent, my_agent])\nprint(\"Success\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed\")","metadata":{"execution":{"iopub.status.busy":"2023-02-21T10:13:13.133087Z","iopub.execute_input":"2023-02-21T10:13:13.133660Z","iopub.status.idle":"2023-02-21T10:13:13.369278Z","shell.execute_reply.started":"2023-02-21T10:13:13.133612Z","shell.execute_reply":"2023-02-21T10:13:13.368074Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"1\n88d\nrandom\n8c2\n8c6\nrandom\ne77\nec9\nrandom\n1dc1a\n7b461de\nrandom\n838bedd39f\n838bedd493\nrandom\n838bedd4a4\n838bf5f096\nrandom\n838bf623d7\nrandom\n23248d7558e03\nrandom\n23248d7e784f4\nrandom\n23248d7e7d1d8\nrandom\n232496275398d\nrandom\n2324bd1f18c41\nrandom\n2324bd7133ac2\nrandom\n2324bd72b8e96\nrandom\n2324d76d3c5b7\nrandom\n2393cefdb4573\nrandom\n262d9c60843d4\nrandom\n26529990ac3be\nrandom\n2652999102bb3\nrandom\n1625193f6ae737\nrandom\n8ca4b7258b4c48\nrandom\n13e6423febbe3e4\nrandom\n13e64242ceb0675\nrandom\n5efbcbb079f0a2f9f\nrandom\n5efbcbb3375c80034\nrandom\n5efbcbb3376a2f2a0\nrandom\n5efbcbcbe034f47e5\nrandom\n7e9e6d9b47eaf0049\nrandom\n93b584258d0eed08a\nrandom\n94e1772d3b90ecde4\nrandom\nSuccess\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nout = sys.stdout\nsub = utils.read_file(\"/kaggle/working/submission.py\")\nprint(callable(\"/kaggle/working/submission.py\"))\nagent = utils.call(submission)\nprint(agent)\nprint(type(agent))\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n    from random import choice\n    pi = '''\\\n    + str(dict(agent.pi)).replace(' ', '')\\\n    + '''\n    state = observation.board\n    state.append(observation.mark)\n    state_key = np.array(state).astype(str) \n    state_key = hex(int(''.join(state_key), 3))[2:]\n    if state_key not in pi.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    \n    actions = list(pi[state_key].keys())\n    probs = list(pi[state_key].values())\n    action = np.random.choice(actions, p=probs).tolist()\n    \n    return action\n    '''\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","metadata":{"execution":{"iopub.status.busy":"2023-02-21T09:35:42.523953Z","iopub.execute_input":"2023-02-21T09:35:42.524357Z","iopub.status.idle":"2023-02-21T09:35:43.209265Z","shell.execute_reply.started":"2023-02-21T09:35:42.524322Z","shell.execute_reply":"2023-02-21T09:35:43.208272Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}